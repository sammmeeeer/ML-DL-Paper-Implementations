{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b8aacc-aa34-4e5f-bc74-1305f9fbac95",
   "metadata": {},
   "source": [
    "### Micrograd in PyTorch\n",
    "- This project is inspired by Andrej Karpathy's Micrograd.\n",
    "- Micrograd is a simplified Autograd system.\n",
    "- This project is intended to help me learn and understand the implementation of simplified autograd.\n",
    "- My code can be a bit retarded tho...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a372cd-87f8-4ee6-92b8-f05fa0ead33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c2451cb-4ffa-416a-a6fd-cca03b60b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\"Stores a single scalar value and its gradient\"\"\"\n",
    "    def __init__(self, data, requires_grad=True):\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            self.data = data.clone().detach().requires_grad_(requires_grad)\n",
    "        else:\n",
    "            self.data = torch.tensor(data, requires_grad=requires_grad)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Value):\n",
    "            return Value(self.data + other.data)\n",
    "        else:\n",
    "            return Value(self.data + other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Value):\n",
    "            return Value(self.data * other.data)\n",
    "        else:\n",
    "            return Value(self.data * other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        return Value(self.data ** other)\n",
    "\n",
    "    def relu(self):\n",
    "        return Value(torch.relu(self.data))\n",
    "\n",
    "    def backward(self):\n",
    "        self.data.backward()\n",
    "\n",
    "    def __neg__(self):\n",
    "        return Value(-self.data)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other \n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return Value(other) + (-self)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * (other ** -1)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return Value(other) * (self ** -1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data.item()}, grad={self.data.grad})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa15c03-3b84-4b7b-a143-c826be09fd59",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58704857-88bd-4414-bbab-f787b42a3a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e324d12-96aa-4484-ab8c-48dea2b9f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module, self).__init__()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = None \n",
    "\n",
    "    def parameters(self):\n",
    "        return list(self.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68a7f017-4c33-47d6-8fab-365d52f063f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(Module):\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        super(Neuron, self).__init__()\n",
    "        self.w = nn.Parameter(torch.randn(nin))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "        self.nonlin = nonlin \n",
    "\n",
    "    def forward(self, X):\n",
    "        act = torch.sum(self.w * x, dim=0) + self.b # Activation \n",
    "        if self.nonlin:\n",
    "            return torch.relu(act)\n",
    "        else:\n",
    "            return act \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c34372e-a924-423d-9be9-4cf11fe617d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Module):\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        super(Layer, self).__init__()\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2e370b7-8a9f-4cf1-9ec6-61f96a0fe9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts):\n",
    "        super(MLP, self).__init__()\n",
    "        sz = [nin] + nouts \n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "510510d2-55b4-4948-afc0-6554fa18b3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP of [Layer of [ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784), ReLUNeuron(784)], Layer of [LinearNeuron(256), LinearNeuron(256), LinearNeuron(256), LinearNeuron(256), LinearNeuron(256), LinearNeuron(256), LinearNeuron(256), LinearNeuron(256), LinearNeuron(256), LinearNeuron(256)]]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(784, [256, 10])\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a8f437-1e72-4e16-8dbc-07de01ce5a30",
   "metadata": {},
   "source": [
    "#### The above output is kinda gibberish but...\n",
    "- 'MLP of [...]' This indicates that the network is multi-layer-perceptron\n",
    "- 'LinearNeuron(256)' This indicates that the second layer has 256 'Layer of [...]]'\n",
    "- 'ReLUNeuron(784)': This indicates that the first layer has 784 neurons and uses the ReLU activation of function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8cede8-5c2f-40df-b8ef-bc413b00e736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
